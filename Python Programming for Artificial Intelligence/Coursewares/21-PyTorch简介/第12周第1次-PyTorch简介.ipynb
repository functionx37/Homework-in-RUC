{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3563e-19, 1.3563e-19, 4.5071e+16, 2.4756e-12],\n",
       "        [4.1183e-11, 1.1704e-19, 1.3563e-19, 1.3563e-19],\n",
       "        [4.5071e+16, 2.4756e-12, 4.1411e-11, 1.1704e-19]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "t.Tensor(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2683, 0.1495, 0.9698, 0.5211],\n",
       "         [0.8555, 0.4007, 0.6352, 0.3024],\n",
       "         [0.4550, 0.2391, 0.5623, 0.3211]],\n",
       "\n",
       "        [[0.0252, 0.3131, 0.9875, 0.1003],\n",
       "         [0.0702, 0.2443, 0.2608, 0.8466],\n",
       "         [0.4603, 0.7733, 0.1387, 0.1822]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成一个随机Tensor\n",
    "x = t.rand(2, 3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "2 3 4\n"
     ]
    }
   ],
   "source": [
    "# 查看x的形状\n",
    "print(x.size())\n",
    "\n",
    "print(x.size()[0], x.size()[1], x.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9847, 1.0203, 1.6382, 1.1281],\n",
       "         [1.4748, 0.8412, 0.8719, 1.0463],\n",
       "         [0.7840, 0.8364, 1.5326, 0.5811]],\n",
       "\n",
       "        [[0.1999, 1.1234, 1.9369, 0.2544],\n",
       "         [0.2200, 0.5460, 1.0673, 1.8135],\n",
       "         [0.8517, 1.3583, 1.0541, 0.6716]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#两个Tensor相加\n",
    "y = t.rand(2, 3, 4)\n",
    "z = x + y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-82390817fe76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 注意两个张量对应的维度要匹配\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# 注意两个张量对应的维度要匹配\n",
    "y1 = t.rand(3, 4, 2)\n",
    "z1 = x + y1\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9847, 1.0203, 1.6382, 1.1281],\n",
      "         [1.4748, 0.8412, 0.8719, 1.0463],\n",
      "         [0.7840, 0.8364, 1.5326, 0.5811]],\n",
      "\n",
      "        [[0.1999, 1.1234, 1.9369, 0.2544],\n",
      "         [0.2200, 0.5460, 1.0673, 1.8135],\n",
      "         [0.8517, 1.3583, 1.0541, 0.6716]]])\n"
     ]
    }
   ],
   "source": [
    "#通过函数add实现加法\n",
    "print(t.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9847, 1.0203, 1.6382, 1.1281],\n",
       "         [1.4748, 0.8412, 0.8719, 1.0463],\n",
       "         [0.7840, 0.8364, 1.5326, 0.5811]],\n",
       "\n",
       "        [[0.1999, 1.1234, 1.9369, 0.2544],\n",
       "         [0.2200, 0.5460, 1.0673, 1.8135],\n",
       "         [0.8517, 1.3583, 1.0541, 0.6716]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#另外一个写法，结果输出到提前定义好的Tensor中\n",
    "result = t.Tensor(2, 3, 4)\n",
    "t.add(x, y, out = result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2683, 0.1495, 0.9698, 0.5211],\n",
      "         [0.8555, 0.4007, 0.6352, 0.3024],\n",
      "         [0.4550, 0.2391, 0.5623, 0.3211]],\n",
      "\n",
      "        [[0.0252, 0.3131, 0.9875, 0.1003],\n",
      "         [0.0702, 0.2443, 0.2608, 0.8466],\n",
      "         [0.4603, 0.7733, 0.1387, 0.1822]]])\n",
      "tensor([[[0.2683, 0.1495, 0.9698, 0.5211],\n",
      "         [0.8555, 0.4007, 0.6352, 0.3024],\n",
      "         [0.4550, 0.2391, 0.5623, 0.3211]],\n",
      "\n",
      "        [[0.0252, 0.3131, 0.9875, 0.1003],\n",
      "         [0.0702, 0.2443, 0.2608, 0.8466],\n",
      "         [0.4603, 0.7733, 0.1387, 0.1822]]])\n",
      "tensor([[[0.9847, 1.0203, 1.6382, 1.1281],\n",
      "         [1.4748, 0.8412, 0.8719, 1.0463],\n",
      "         [0.7840, 0.8364, 1.5326, 0.5811]],\n",
      "\n",
      "        [[0.1999, 1.1234, 1.9369, 0.2544],\n",
      "         [0.2200, 0.5460, 1.0673, 1.8135],\n",
      "         [0.8517, 1.3583, 1.0541, 0.6716]]])\n"
     ]
    }
   ],
   "source": [
    "# 函数名带下划线 x.add_(y)会改变x，不带下划线x.add(y) 返回一个新的Tensor，x不改变\n",
    "print(x)\n",
    "x.add(y)\n",
    "print(x)\n",
    "x.add_(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2., -2., -2.],\n",
      "        [-2., -2., -2.]])\n"
     ]
    }
   ],
   "source": [
    "a =t.ones(2, 3)\n",
    "b = 3.0 * t.ones(2, 3)\n",
    "c = a - b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d8132db88a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "d = t.ones(2,2)\n",
    "e = a - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "tensor([[ 2.,  8.],\n",
      "        [18., 32.],\n",
      "        [50., 72.]])\n",
      "tensor(182.)\n",
      "tensor([[ 10.,  22.,  34.],\n",
      "        [ 22.,  50.,  78.],\n",
      "        [ 34.,  78., 122.]])\n",
      "tensor([[ 70.,  88.],\n",
      "        [ 88., 112.]])\n"
     ]
    }
   ],
   "source": [
    "a = t.Tensor([[1,2], [3,4], [5, 6]])\n",
    "print(a)\n",
    "\n",
    "#数乘矩阵\n",
    "b = 2.0 * a\n",
    "print(b)\n",
    "\n",
    "#对应点相乘，sum后即为卷积\n",
    "c = a * b\n",
    "print(c)\n",
    "print(c.sum())\n",
    "\n",
    "# a: 3 * 2, b.t():2 * 3\n",
    "# d: 3 * 3\n",
    "d = a.mm(b.t())\n",
    "print(d)\n",
    "\n",
    "\n",
    "# a.t(): 2 * 3, b.t():3 * 2\n",
    "# e: 2 * 2\n",
    "e = a.t().mm(b)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5., 11., 17., 23.],\n",
      "        [11., 25., 39., 53.],\n",
      "        [17., 39., 61., 83.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 4x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ad4f550863e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#如果矩阵大小不满足 x: i * n, y:n * j的方式，则出错\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 4x2)"
     ]
    }
   ],
   "source": [
    "a = t.Tensor([[1,2], [3,4], [5, 6]]) # 3 * 2\n",
    "b = t.Tensor([[1, 2],[3,4],[5,6],[7,8]]) # 4 * 2\n",
    "\n",
    "c = a.mm(b.t()) # 3 * 4 \n",
    "print(c)\n",
    "\n",
    "\n",
    "#如果矩阵大小不满足 x: i * n, y:n * j的方式，则出错\n",
    "c = a.mm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[1., 2.]])\n",
      "tensor([[ 5.],\n",
      "        [11.],\n",
      "        [17.]])\n",
      "tensor([[ 5., 11., 17.]])\n"
     ]
    }
   ],
   "source": [
    "#A: 3 * 2\n",
    "#x: 1 * 2\n",
    "A = t.Tensor([[1,2], [3,4], [5, 6]]) \n",
    "x = t.Tensor([[1, 2]])\n",
    "print(A)\n",
    "print(x)\n",
    "\n",
    "# A: 3*2; b.t(): 2 * 1\n",
    "# 结果：3 * 1\n",
    "c = A.mm(x.t())\n",
    "print(c)\n",
    "\n",
    "#x: 1 * 2, A.t(): 2 * 3\n",
    "# 结果: 1 * 3\n",
    "d = x.mm(A.t())\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4., 5., 6.]])\n",
      "tensor([[1., 2., 3., 4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "a=t.Tensor([[[1,2,3],[4,5,6]]])\n",
    "b=t.Tensor([1,2,3,4,5,6])\n",
    "\n",
    "print(a.view(1,6))\n",
    "print(b.view(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3., 4.],\n",
      "         [5., 6., 7., 8.]]])\n",
      "---------------------------------\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.],\n",
      "        [7., 8.]])\n",
      "---------------------------------\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "a=t.Tensor([[[1,2,3, 4],[5,6,7,8]]])\n",
    "print(a)\n",
    "print('---------------------------------')\n",
    "print(a.view(4,2))\n",
    "print('---------------------------------')\n",
    "print(a.view(2, 2 ,2))\n",
    "print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8445, 0.8684, 0.2850, 0.7711],\n",
      "         [0.1709, 0.6919, 0.9432, 0.8347],\n",
      "         [0.0158, 0.4547, 0.9059, 0.6666]],\n",
      "\n",
      "        [[0.2744, 0.8219, 0.0331, 0.1305],\n",
      "         [0.8894, 0.8079, 0.1059, 0.3543],\n",
      "         [0.7605, 0.5903, 0.2350, 0.2651]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8445, 0.8684, 0.2850, 0.7711])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor的选取操作与numpy类似\n",
    "x = t.rand(2, 3, 4)\n",
    "\n",
    "print(x)\n",
    "x[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8445, 0.2744])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8445, 0.1709, 0.0158],\n",
       "        [0.2744, 0.8894, 0.7605]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9432, 0.8347],\n",
       "         [0.9059, 0.6666]],\n",
       "\n",
       "        [[0.1059, 0.3543],\n",
       "         [0.2350, 0.2651]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1:3, 2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyTorch Tensor与Numpy之间的转换\n",
    "#tensor->numpy\n",
    "a = t.ones(2, 3, 4)\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# numpy --> Tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = t.from_numpy(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# 上述转换中，Tensor和Numpy共享内容，所以修改一个另一个也随之改变\n",
    "\n",
    "print(a)\n",
    "b.add_(1) #注意是下划线add_\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.5609, 1.7392, 0.9534, 1.3781],\n",
      "         [0.7902, 1.1324, 1.1799, 1.5786],\n",
      "         [0.3449, 1.0520, 1.8761, 0.9267]],\n",
      "\n",
      "        [[0.4491, 1.6322, 0.9825, 0.2846],\n",
      "         [1.0392, 1.1096, 0.9125, 1.3212],\n",
      "         [1.1519, 1.1753, 1.1504, 0.7545]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#支持GPU，将x和y都转移到GPU中进行运算\n",
    "# 在大规模数据复杂Tensor运算时，具有优势；\n",
    "# 小规模数据时由于具有数据转移开销，CPU会更快一点\n",
    "\n",
    "#本机器不支持GPU，不会运行以下代码\n",
    "if t.cuda.is_available():\n",
    "    x = x.cuda() #将张量x转移到GPU中\n",
    "    y = y.cuda() #将张量y转移到GPU中\n",
    "    z = x + y #在GPU中运算x + y\n",
    "else:\n",
    "    z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.5609, 1.7392, 0.9534, 1.3781],\n",
       "         [0.7902, 1.1324, 1.1799, 1.5786],\n",
       "         [0.3449, 1.0520, 1.8761, 0.9267]],\n",
       "\n",
       "        [[0.4491, 1.6322, 0.9825, 0.2846],\n",
       "         [1.0392, 1.1096, 0.9125, 1.3212],\n",
       "         [1.1519, 1.1753, 1.1504, 0.7545]]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如果不进行判断，x.cuda()语句在没有GPU环境的情况下出错\n",
    "x = x.cuda() #将张量x转移到GPU中\n",
    "y = y.cuda() #将张量y转移到GPU中\n",
    "x + y #在GPU中运算x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# 求函数 y= x^2 在x=3时的导数: y'(3) = dy/dx|x=3\n",
    "\n",
    "x=t.tensor(3.0,requires_grad=True)\n",
    "y=x.mul(x)\n",
    "\n",
    "#判断x,y是否是可以求导的\n",
    "print(x.requires_grad)\n",
    "print(y.requires_grad)\n",
    "\n",
    "#求导，通过backward函数来实现\n",
    "y.backward()\n",
    "\n",
    "#查看导数，也即所谓的梯度\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# 练习\n",
    "# 求y = max(0, x)在x=1和x=-1处的导数\n",
    "\n",
    "\n",
    "#定义x=1.0 和y=max(0, x)\n",
    "x = t.tensor(1.0, requires_grad=True)\n",
    "y = t.max(x, t.zeros(1,1))\n",
    "\n",
    "#调用backward并打印导数\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# 改变x后需要重新执行y和backward\n",
    "x = t.tensor(-1.0, requires_grad=True)\n",
    "y = t.max(x, t.zeros(1,1))\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-4.)\n",
      "tensor(-1.)\n",
      "tensor(-0.2500)\n"
     ]
    }
   ],
   "source": [
    "# 求y = 1/x\n",
    "x = t.tensor(0.5, requires_grad=True)\n",
    "y = t.reciprocal(x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "x = t.tensor(1.0, requires_grad=True)\n",
    "y = t.reciprocal(x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "x = t.tensor(2., requires_grad=True)\n",
    "y = t.reciprocal(x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.4142)\n",
      "tensor(-0.5000)\n",
      "tensor(-0.1768)\n"
     ]
    }
   ],
   "source": [
    "# 求y = 1/\\sqrt(x)\n",
    "x = t.tensor(0.5, requires_grad=True)\n",
    "y = t.reciprocal( t.sqrt(x))\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "x = t.tensor(1.0, requires_grad=True)\n",
    "y = t.reciprocal( t.sqrt(x))\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "x = t.tensor(2.0, requires_grad=True)\n",
    "y = t.reciprocal( t.sqrt(x))\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# 求y = 1/\\sqrt(x)\n",
    "x = t.tensor(1.0, requires_grad=True)\n",
    "g = t.sqrt(x)\n",
    "y = t.reciprocal(g)\n",
    "\n",
    "g.backward(retain_graph=True)\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "y.backward(retain_graph=True)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "# f(x,y) = x^2 + 2 * y^2 + xy\n",
    "# 在 (1.0, 1.0)处的导数\n",
    "x = t.tensor(1.0, requires_grad=True)\n",
    "y = t.tensor(1.0, requires_grad=True)\n",
    "\n",
    "f = x.pow(2) + t.tensor(2.0).mul(y.pow(2)) + x.mul(y)\n",
    "f.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0064)\n",
      "tensor(0.0471)\n",
      "tensor(0.9465)\n"
     ]
    }
   ],
   "source": [
    "# f(x,y, z) = ln(e^x + e^y + e^z)\n",
    "# 在 (0.0, 2.0, 5.0)处的导数\n",
    "x = t.tensor(0.0, requires_grad=True)\n",
    "y = t.tensor(2.0, requires_grad=True)\n",
    "z = t.tensor(5.0, requires_grad=True)\n",
    "\n",
    "f = t.log(t.exp(x) + t.exp(y) + t.exp(z))\n",
    "f.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.]], requires_grad=True) tensor([[1., 1.],\n",
      "        [1., 2.]]) tensor([[1., 0.]])\n",
      "\n",
      "tensor([[14.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([[ 7., 10.]])\n"
     ]
    }
   ],
   "source": [
    "# 注意：把x和b定义为一个1*2的矩阵，x.mm()是矩阵乘法\n",
    "x = t.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "\n",
    "A = t.tensor([[1.0, 1.0], \n",
    "              [1.0, 2.0]])\n",
    "b = t.tensor([[1.0, 0.0]])\n",
    "print(x, A, b)\n",
    "print()\n",
    "\n",
    "f = x.mm(A).mm(x.t()) + x.mm(b.t())\n",
    "print(f)\n",
    "print()\n",
    "f.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.]], requires_grad=True) tensor([[3., 4.]], requires_grad=True) tensor([[1., 1.],\n",
      "        [1., 2.]]) tensor([[1., 0.]])\n",
      "\n",
      "tensor([[30.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([[ 8., 11.]])\n",
      "tensor([[3., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# 注意：把x和b定义为一个1*2的矩阵，x.mm()是矩阵乘法\n",
    "x = t.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "y = t.tensor([[3.0, 4.0]], requires_grad=True)\n",
    "\n",
    "A = t.tensor([[1.0, 1.0], \n",
    "              [1.0, 2.0]])\n",
    "b = t.tensor([[1.0, 0.0]])\n",
    "print(x, y, A, b)\n",
    "print()\n",
    "\n",
    "f = x.mm(A).mm(y.t()) + x.mm(b.t())\n",
    "print(f)\n",
    "print()\n",
    "f.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.]]) tensor([[3., 4.]]) tensor([[ 2., 12.],\n",
      "        [ 1.,  2.]], requires_grad=True) tensor([[1., 0.]])\n",
      "\n",
      "tensor([[77.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([[3., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# f(A) = x A A^T y \n",
    "x = t.tensor([[1.0, 2.0]])\n",
    "y = t.tensor([[3.0, 4.0]])\n",
    "\n",
    "A = t.tensor([[2.0, 12.0], \n",
    "              [1.0, 2.0]], requires_grad=True)\n",
    "b = t.tensor([[1.0, 0.0]])\n",
    "print(x, y, A, b)\n",
    "print()\n",
    "\n",
    "f = x.mm(A).mm(y.t()) + x.mm(b.t())\n",
    "print(f)\n",
    "print()\n",
    "f.backward()\n",
    "#print(x.grad)\n",
    "#print(y.grad)\n",
    "print(A.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
